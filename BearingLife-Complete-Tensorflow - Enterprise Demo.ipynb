{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bearing Life Prediction Experiment\n",
        "This is an example of using the the Seeq/Python module for applying Machine Learning models to cleansed data from Seeq."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Setup\r\n",
        "## Install dependencies"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install seaborn\n",
        "!{sys.executable} -m pip install seeq"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616672638533
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from numpy.random import seed\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense \n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from seeq import spy"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696292546
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace \r\n",
        "from azureml.core import Experiment\r\n",
        "from azureml.mlflow import register_model\r\n",
        "from azureml.core import Dataset, Datastore\r\n",
        "from azureml.data.datapath import DataPath\r\n",
        "\r\n",
        "experiment_name = 'Bearing_Failure_Prediction'\r\n",
        "ws = Workspace.from_config()\r\n",
        "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616696310912
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\r\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)\r\n",
        "run = experiment.start_logging()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616696330391
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the keyvault associated with this notebook\r\n",
        "from azureml.core import Workspace\r\n",
        "ws = Workspace.from_config()\r\n",
        "keyvault = ws.get_default_keyvault()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616696343621
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log into Seeq with SPy"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the credentials from the secrets store\r\n",
        "access_key_name = keyvault.get_secret(name=\"accesskey\")\r\n",
        "access_pwd = keyvault.get_secret(name=\"pwd\")\r\n",
        "\r\n",
        "# Login to Seeq\r\n",
        "spy.login(url='https://explore.seeq.com/', username=access_key_name, password=access_pwd, ignore_ssl_errors=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616703991649
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search for Bearing Signals"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "vibration_signals = spy.search({\n",
        "    'Name': '/^Vibration$/',\n",
        "    'Path': 'Compressors Vibration Probes >> C23',\n",
        "    'Scoped To': '7CC84C64-A14A-4DA2-AA6F-4C56733FADB2'\n",
        "},)\n",
        "# workbook = 'Seeq Integration with Python Machine Learning'\n",
        "vibration_signals"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "gather": {
          "logged": 1616696377300
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the condition giving time periods of normal operation..."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "bearing_norm_operation = spy.search({\n",
        "    'Name': '/^Training Window$/',\n",
        "    'Scoped To': '7CC84C64-A14A-4DA2-AA6F-4C56733FADB2'\n",
        "},)\n",
        "# workbook = 'Seeq Integration with Python Machine Learning'\n",
        "bearing_norm_operation = bearing_norm_operation.iloc[0]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696394907
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pull Bearing Signal Data for the Window"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "First pull the Bearing Normal Operation Condition to find the timeperiod over which to retrive the data to train the model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_cond = spy.pull(bearing_norm_operation,start='2020-04-01 12:00:00', end='2020-07-15 12:00:00', grid=None)\n",
        "start_time, end_time = df_cond.loc[0,['Capsule Start', 'Capsule End']]\n",
        "df_cond"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696417366
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next pull the bearing vibration data."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = spy.pull(vibration_signals, start=start_time, end=end_time, grid=None)\n",
        "# create tabular dataset from Parquet files in datastore\n",
        "datastore = ws.get_default_datastore()\n",
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696425988
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot(figsize=(24,6), rot=60)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696441535
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register the dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dataset = Dataset.Tabular.register_pandas_dataframe(df, datastore, name=\"bearing_life_training_data_set\", description=\"Vibration data for training\", show_progress=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616696461073
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Develop the Machine Learning Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardize data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Standardization\n",
        "names = df.columns\n",
        "index = df.index\n",
        "scaler = StandardScaler().fit(df)\n",
        "scaled_df = pd.DataFrame(scaler.transform(df), index=index, columns=names)\n",
        "pickle.dump(scaler, open('scaler.pkl','wb'))\n",
        "run.upload_file(name='bearing_failure_exp_scaler', path_or_stream='scaler.pkl')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696471334
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_df.plot(figsize=(12,6), rot=60)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696478118
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Artificial Neural Network"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "seed(9) # global seed for numpy\n",
        "tf.random.set_seed(10) # global seed for tensorflow\n",
        "activation = 'elu'\n",
        "run.log(\"activation\", activation)\n",
        "\n",
        "# Input layer:\n",
        "model=Sequential()\n",
        "# First hidden layer, connected to input vector X. \n",
        "model.add(Dense(10,activation=activation,\n",
        "                kernel_initializer='glorot_uniform',\n",
        "                kernel_regularizer=regularizers.l2(0.0),\n",
        "                input_shape=(scaled_df.shape[1],)\n",
        "               )\n",
        "         )\n",
        "\n",
        "model.add(Dense(2,activation=activation,\n",
        "                kernel_initializer='glorot_uniform'))\n",
        "\n",
        "model.add(Dense(10,activation=activation,\n",
        "                kernel_initializer='glorot_uniform'))\n",
        "\n",
        "model.add(Dense(scaled_df.shape[1],\n",
        "                kernel_initializer='glorot_uniform'))\n",
        "\n",
        "model.compile(loss='mse',optimizer='adam')\n",
        "run.log(\"optimizer\", 'adam')\n",
        "run.log(\"kernel_initalizer\", 'glorot_uniform')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696494356
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model for 100 epochs, batch size of 10: \n",
        "NUM_EPOCHS=100\n",
        "BATCH_SIZE=10\n",
        "X_train = scaled_df  # .sample(frac=1)\n",
        "history=model.fit(np.array(X_train),np.array(X_train),\n",
        "                  batch_size=BATCH_SIZE, \n",
        "                  epochs=NUM_EPOCHS,\n",
        "                  validation_split=0.05,\n",
        "                  verbose = 1)\n",
        "model.save('bearing_model.h5')\n",
        "run.upload_file(name='bearing_failure_exp_model', path_or_stream='bearing_model.h5')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696518172
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(history.history['loss'], 'b', label='Training loss')\n",
        "plt.plot(history.history['val_loss'], 'r', label='Validation loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss, [mse]')\n",
        "plt.show()\n",
        "run.log_image('Loss', plot=plt)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696533293
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the mean absolute errors and look at their distribution in the training set"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_pred = model.predict(np.array(scaled_df))\n",
        "X_pred = pd.DataFrame(X_pred, columns=scaled_df.columns)\n",
        "X_pred.index = scaled_df.index\n",
        "\n",
        "scored = pd.DataFrame(index=scaled_df.index)\n",
        "scored['Loss_mae'] = np.mean(np.abs(X_pred-scaled_df), axis = 1)\n",
        "plt.figure()\n",
        "sns.histplot(scored['Loss_mae'],\n",
        "             bins = 10, \n",
        "             kde= True,\n",
        "            color = 'blue');\n",
        "run.log_image('Loss MAE', plot=plt)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696552171
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scored.plot(figsize=(12,6), rot=60)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696565448
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pick a threshold above the noise level from the above distribution"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# pick a threshold above the noise level from the above distribution\n",
        "thrs = 1.6\n",
        "run.log(\"threshold\", thrs)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696580943
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply ML Trained Model to New Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a Data Scientist you might want to validate your model though with test data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Aux function to visualize MAE with respect to selected threshold\n",
        "def loss_threshold_visualization(new_df, threshold, predictor):\n",
        "    X_pred = predictor.predict(np.array(new_df))\n",
        "    X_pred = pd.DataFrame(X_pred, columns=new_df.columns)\n",
        "    X_pred.index = new_df.index\n",
        "\n",
        "    scored = pd.DataFrame(index=new_df.index)\n",
        "    scored['Loss_mae'] = np.mean(np.abs(X_pred-new_df), axis = 1)\n",
        "    scored['Threshold'] = threshold\n",
        "    scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
        "    return scored"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696596720
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to apply the trained model to new data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def anomaly_detection(new_df, threshold, predictor):\n",
        "    X_pred = predictor.predict(np.array(new_df))\n",
        "    X_pred = pd.DataFrame(X_pred, columns=new_df.columns)\n",
        "    X_pred.index = new_df.index\n",
        "\n",
        "    loss = np.mean(np.abs(X_pred-new_df), axis = 1)\n",
        "    score = loss > threshold\n",
        "    return pd.DataFrame(score * 1, index=new_df.index, columns=['BearingStatus'])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696600384
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate the model\r\n",
        "We can validate with data outside of the training period. Make another call to get the data for a larger time period"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a new call to Seeq for new data \n",
        "new_df = spy.pull(vibration_signals, start='2020-04-15 12:00:00', end='2020-07-05 12:00:00', grid=None)\n",
        "new_df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696606788
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale the new df but make sure it is scaled using the previous parameters from the scaler used for the training set"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# It is important to used the previously defined standarization scaler\n",
        "new_scaled_df = pd.DataFrame(scaler.transform(new_df), \n",
        "                             index=new_df.index, \n",
        "                             columns=new_df.columns)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696612911
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to visualize MAE with respect to the chosen threshold\n",
        "loss_level = loss_threshold_visualization(new_scaled_df, thrs, model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696617965
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_level.plot(figsize=(12,6), rot=60)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696621997
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create and push a new signal with the status of the bearing health\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Status 0 means \"normal\" Status 1 means \"abnormal\""
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "status = anomaly_detection(new_scaled_df, thrs, model)\n",
        "output_dataset = Dataset.Tabular.register_pandas_dataframe(status, datastore, name=\"bearing_life_output_set\", description=\"Output after training\", show_progress=True)\n",
        "status.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696633072
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "status.plot(figsize=(12,6), rot=60)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616696641946
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register the Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Model\r\n",
        "from azureml.core.resource_configuration import ResourceConfiguration\r\n",
        "details = run.get_details()\r\n",
        "runId = details['runId']\r\n",
        "\r\n",
        "model = run.register_model(model_name='bearing-life-tensorflow-model',\r\n",
        "                          model_path='bearing_failure_exp_model', \r\n",
        "                          model_framework=Model.Framework.TENSORFLOW, \r\n",
        "                          datasets=[('training_data_set', input_dataset),('output_data_set', output_dataset)],\r\n",
        "                          model_framework_version=keras.__version__, \r\n",
        "                          description='Bearing life model',\r\n",
        "                          tags={'area': 'Bearing Life', 'type': 'ANN'})\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616696666337
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.update(sample_input_dataset=input_dataset, sample_output_dataset=output_dataset)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616672788173
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push signal back to Seeq"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a status signal in Seeq\r\n",
        "\r\n",
        "spy.push(status, metadata=pd.DataFrame({\r\n",
        "    'Interpolation Method': {\r\n",
        "        'BearingStatus': 'step'\r\n",
        "    },\r\n",
        "    'Type': 'Signal',\r\n",
        "    'Name': 'Bearing Status',\r\n",
        "    'Model Version': model.version\r\n",
        "}), workbook='7CC84C64-A14A-4DA2-AA6F-4C56733FADB2', worksheet = '7. Bearing Status Prediction Output')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616696709275
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.complete()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616696715336
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.6-final",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python3"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}